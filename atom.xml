<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>哲人存远</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://gggcy.github.io/"/>
  <updated>2019-08-09T01:21:27.492Z</updated>
  <id>http://gggcy.github.io/</id>
  
  <author>
    <name>Cunyuan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hello World</title>
    <link href="http://gggcy.github.io/uncategorized/hello-world"/>
    <id>http://gggcy.github.io/uncategorized/hello-world</id>
    <published>2019-08-08T03:31:26.034Z</published>
    <updated>2019-08-09T01:21:27.492Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
      <category term="随笔" scheme="http://gggcy.github.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>markdown格式编写</title>
    <link href="http://gggcy.github.io/uncategorized/Joint"/>
    <id>http://gggcy.github.io/uncategorized/Joint</id>
    <published>2019-06-08T07:30:28.000Z</published>
    <updated>2019-08-08T08:49:19.714Z</updated>
    
    <content type="html"><![CDATA[<h1 id="markdown格式编写"><a href="#markdown格式编写" class="headerlink" title="markdown格式编写"></a>markdown格式编写</h1><a id="more"></a><h2 id="第一个标题"><a href="#第一个标题" class="headerlink" title="第一个标题"></a>第一个标题</h2><p>段落1 中间空一行</p><p>段落2</p><h2 id="区块引用"><a href="#区块引用" class="headerlink" title="区块引用"></a>区块引用</h2><blockquote><p>使用“&gt;”和空格高亮</p><p>这是一段段落高亮</p></blockquote><h2 id="超链接"><a href="#超链接" class="headerlink" title="超链接"></a>超链接</h2><p><a href="www.baidu.com">百度</a>  利用中括号小括号的格式</p><h2 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h2><p><img src="http://image.baidu.com/search/detail?ct=503316480&amp;z=undefined&amp;tn=baiduimagedetail&amp;ipn=d&amp;word=robomaster%E7%9B%B4%E6%92%AD&amp;step_word=&amp;ie=utf-8&amp;in=&amp;cl=2&amp;lm=-1&amp;st=undefined&amp;hd=undefined&amp;latest=undefined&amp;copyright=undefined&amp;cs=3125832727,3597497147&amp;os=1898171743,3878211660&amp;simid=3369673952,459146154&amp;pn=1&amp;rn=1&amp;di=20790&amp;ln=361&amp;fr=&amp;fmq=1565251372619_R&amp;fm=&amp;ic=undefined&amp;s=undefined&amp;se=&amp;sme=&amp;tab=0&amp;width=undefined&amp;height=undefined&amp;face=undefined&amp;is=0,0&amp;istype=0&amp;ist=&amp;jit=&amp;bdtype=0&amp;spn=0&amp;pi=0&amp;gsm=0&amp;objurl=http%3A%2F%2Fi.dyt8.cc%2F66%2Fab%2F0a%2Ff1%2F38%2F38%2F4d%2Fe5%2Fa2%2Fd2%2F4d%2F0d%2Fb8%2Fc7%2F62%2Ffb.jpg&amp;rpstart=0&amp;rpnum=0&amp;adpicid=0&amp;force=undefined" alt="风景"> 超链接前面+!</p><h2 id="无序列表"><a href="#无序列表" class="headerlink" title="无序列表"></a>无序列表</h2><ul><li>1</li><li>1</li><li>1 “*”</li></ul><ul><li>2</li><li>2</li><li>2 “-“</li></ul><h2 id="有序列表"><a href="#有序列表" class="headerlink" title="有序列表"></a>有序列表</h2><ol><li>“1.”<br>2.<br>3.</li></ol><h2 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h2><hr><hr><p>“ *或是-” 大于3个</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;markdown格式编写&quot;&gt;&lt;a href=&quot;#markdown格式编写&quot; class=&quot;headerlink&quot; title=&quot;markdown格式编写&quot;&gt;&lt;/a&gt;markdown格式编写&lt;/h1&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Joint Detection and Identification Feature Learning for Person Search</title>
    <link href="http://gggcy.github.io/uncategorized/Joint%20Detection%20and%20Identification%20Feature%20Learning%20for%20Person%20Search"/>
    <id>http://gggcy.github.io/uncategorized/Joint Detection and Identification Feature Learning for Person Search</id>
    <published>2018-10-21T02:27:11.000Z</published>
    <updated>2019-08-10T02:03:19.479Z</updated>
    
    <content type="html"><![CDATA[<p>论文：Joint Detection and Identification Feature Learning for Person Search（整合行人检测和行人再识别的行人搜索新框架）<br>链接：<a href="https://arxiv.org/abs/1604.01850" target="_blank" rel="noopener">https://arxiv.org/abs/1604.01850</a><br>代码地址： <a href="https://github.com/ShuangLI59/person_search" target="_blank" rel="noopener">https://github.com/ShuangLI59/person_search</a> caffe版本<br>疑似复现地址：<a href="https://github.com/liliangqi/person_search" target="_blank" rel="noopener">https://github.com/liliangqi/person_search</a> pytorch版本<br><a id="more"></a></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p> 现有的行人再识别方法中所使用的标准和方法主要关注的是经过裁剪的行人照片，这与现实生活场景中的图片有所不同。本文为了缩小两者的差距，提出了一种行人搜索的新框架，将行人检测和行人再识别结合起来，利用单个CNN来进行训练。文中使用了OIM损失（Online Instance Matching）来训练网络，它比一般的Softmax损失函数的效果更快更好。</p><div align="center"><img src="https://imgchr.com/i/eLZnDP" alt></div>    <p> 同时提出了一个大型的数据集，总共18184 images，所有图像中总共有8432 persons，99809个标注的bounding boxes。  </p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p> 行人重识别和行人搜索的不同点在于，行人重识别，是在已经假设行人检测已经做的完美的基础上做的再识别研究。而行人搜索更贴近于现实世界的应用，更具有挑战性，正如检测行人时可能不可避免的出现false alarms，漏检，misalignments，这些都会对行人搜索的效果产生影响。<br> 与传统方法中将行人检测和再识别分成两个问题不同，本文利用单个CNN将两者结合来解决上述问题，该CNN分为两个部分，一个是pedestrian proposal net，来产生候选行人的 bounding boxes，另一个是identification net，来提取特征来进行与检索目标的比较。两者在 joint optimization过程中具有相互适应的特点，从而消除自身外另一网络带来的问题。<br> 传统的re-id特征学习主要依赖于pair-wise或triplet distance loss functions,它们一次都只能用少量的数据做比较;以及Softmax loss function,虽然把行人重识别问题当成是多分类问题去做，解决了之前两个loss function的问题，但是随着行人类型（不同身份的人）数量的增多，训练一个如此庞大的Softmax 分类器会变得及其的慢，甚至更糟糕的时候网络会无法收敛。</p><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h1><h2 id="2-1-Person-re-id"><a href="#2-1-Person-re-id" class="headerlink" title="2.1 Person re-id"></a>2.1 Person re-id</h2><p>pass</p><h2 id="2-2-pedestrian-detection"><a href="#2-2-pedestrian-detection" class="headerlink" title="2.2 pedestrian detection"></a>2.2 pedestrian detection</h2><p>pass</p><h1 id="3-Method"><a href="#3-Method" class="headerlink" title="3. Method"></a>3. Method</h1><div align="center"><img src="https://imgchr.com/i/eLZmut" alt></div>   <h2 id="Pipline"><a href="#Pipline" class="headerlink" title="Pipline"></a>Pipline</h2><ol><li>首先，经过Stem CNN将raw pixel转换成Convolutional Feature Maps。然后，放到Pedestrain Proposal net生成候选行人的bounding boxes。</li><li>然后放到一个带有Rol(regionof interest) Pooling的识别网络，提取出每个候选区（boundingbox）对应的L2正则化256维特征向量。</li><li>在推测阶段，利用gallery person和目标行人之间的features distances来进行排序。  </li><li>在训练阶段，根据feature vectors，使用OIM loss function以及其他的loss functions来监控identification net，以多任务方式来训练网络。<h2 id="Model-Structure"><a href="#Model-Structure" class="headerlink" title="Model Structure"></a>Model Structure</h2></li><li>文中选用的是ResNet-50作为CNN模型的网络结构，利用其中的conv1和 conv4_3作为stem CNN部分。给定一张输入图片，stem会产生1024个通道的 features maps，它们的分辨率只有原图像的<strong>十六分之一</strong>。  </li><li>根据生成的这些features maps，利用512 × 3 × 3的卷积层来对行人特征进行转换，接着在feature map的每个位置利用<strong>9个anchors</strong>（源于Faster RCNN）和<strong>1个Softmax分类器</strong>来<strong>预测每个anchor是否包含行人在内</strong>，同时还包括了<strong>1个线性回归器</strong>来<strong>调整anchors的位置</strong>。在<strong>NMS</strong>过后仅保留<strong>128个调整后的bounding boxes作为最终的proposals</strong>； </li><li>建立IdentificationNet，用于提取每个候选区的特征，并和目标特征做对比。首先用ROI-Pooling从每个候选区的特征图中池化出一个<strong>1024<em>14</em>14</strong>的区域。然后，他们通过ResNet-50的con4_4到conv5_3，最后通过全局的平均池化层汇总成2048维特征向量。</li><li>另一方面，pedestrian proposal net 生成的 pedestrian proposals 中会不可避免地会包含一些 <strong>false alarms</strong>（也就是proposal里包含的不是行人）和 <strong>misalignments（</strong>也就是框没有对齐行人<strong>）</strong>，因此再次利用Softmax分类器和线性回归器来<strong>拒绝非行人的区域并精修proposal的位置</strong>。另一方面，我们将特征投影到经过L2正则化后的256维向量子空间中，并在推测阶段计算它们和目标行人的<strong>余弦相似度</strong>。<h2 id="Online-Instance-Matching-Loss"><a href="#Online-Instance-Matching-Loss" class="headerlink" title="Online Instance Matching Loss"></a>Online Instance Matching Loss</h2></li></ol><ul><li>labeled identity：与目标行人相吻合的proposal。  </li><li>unlabeled identities：包含行人但不是目标行人的proposal。  </li><li>background clutter：包含非行人物体或者背景的proposal。<br>在OIM损失函数中只考虑前两者。<br>具体见下图：  <div align="center"><img src="https://imgchr.com/i/eLZuHf" alt></div><br>因为目标是为了区分开不同id的人，一个很自然很常用的方法就是<strong>缩小相同id人的特征距离，同时增大不同id的人的特征距离</strong>，为了实现这个目的，我们需要<strong>存储所有人的特征</strong>。对所有训练集图像同时进行网络的前向传递就可以实现，但这对于SGD优化器来说是不实际的，因为随机梯度下降每次只取一个mini-batch，不可能把所有训练集图像一次包含进去训练，所以本文并未选择SGD进行优化，而是采用了 <strong>online approximation</strong>。<br>mini-batch中<strong>一个labeled identity的特征被记为记为x</strong>（x是D维特征向量），之后生成一个 lookup table V（LUT，查询表，前面有介绍过，Figure 3中右上角的蓝色格子），<strong>V用来记录所有labeled identity的特征</strong>（D×L 维矩阵，D是上面的x的维度，也就是一个labeled identity的特征维度；L是不同id的人，就是L个类；所以D×L 维矩阵就是整个LUT的维度）。<br><strong>前向计算</strong>时，会计算minibatch sample 与所有LUT中labeled identity的实例的cosine 相似度，通过矩阵乘法，<strong>后向传播</strong>时，如果query的ID为t，则更新LUT的第t列与之ID对应的labeled identity实例特征，如下：<br><div align="center"><img src="https://img-blog.csdn.net/20180309193201780?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzM2MTQ5MDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt></div><br><div align="center"><img src="https://img-blog.csdn.net/20180309193206569?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzM2MTQ5MDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt></div>  </li></ul><h1 id="4-Dataset"><a href="#4-Dataset" class="headerlink" title="4. Dataset"></a>4. Dataset</h1><h2 id="4-1-CUHK-SYSU"><a href="#4-1-CUHK-SYSU" class="headerlink" title="4.1 CUHK-SYSU"></a>4.1 CUHK-SYSU</h2><p>总共有8432 persons，99809个标注的bounding boxes。<br>For street snaps, 12490 images and 6057 query persons ；<br> For movie/TV，5694 images and 2375 query persons ；<br> training set：11206 images and 5532 query persons ；<br> test set：6978 images and 2900 query persons ；<br> every query至少有与query同一ID的图片，不同的query有不同的gallery set且training and test sets have no overlap on images or query persons，以保证用测试集评价的合理性.  </p><div align="center"><img src="https://imgchr.com/i/eLZZjI" alt></div>       <h2 id="4-2-Metrics"><a href="#4-2-Metrics" class="headerlink" title="4.2 Metrics"></a>4.2 Metrics</h2><ul><li>mAP</li><li>CMC-K</li></ul><h1 id="5-Experiment-Settings"><a href="#5-Experiment-Settings" class="headerlink" title="5. Experiment Settings"></a>5. Experiment Settings</h1><p>文章是基于caffe框架和faster rcnn算法为基础的，用了基于ImageNet 的ResNet50的预训练模型</p><p> 所有用到的loss都有一样的损失权重，每一个mini-batch有2张全景图，学习率初始化为0.001，在4万次迭代后下降到0.0001，然后保持不变直到模型在大约5万次迭代的时候达到收敛。直接看文章开头的代码上手更快些。    </p><p><div align="center">!()[<a href="https://imgchr.com/i/eLZVgA]" target="_blank" rel="noopener">https://imgchr.com/i/eLZVgA]</a></div><br>可以看出不管是CMC或者mAP两项评价指标，<strong>本文提出的方法均遥遥领先于其他方法</strong>。与IDNet方法相同，提升的原因，主要是：检测和重识别两项任务的有效结合，互相提升，以及在OIM loss中有效地使用了unlabeled identity 来监督学习。行人检测中 <strong>detectors 检测器的选择对行人重识别任务的效果提升尤为关键</strong>，我们可以看到CCF、ACF、CNN和GT之间的差别都是巨大的，直接使用现有的 detector 对现实生活中的行人检测任务来说并不是最优的，否则 detector 很可能就会成为限制行人重识别任务效果的一个瓶颈。</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>pass </p><h1 id="个人心得"><a href="#个人心得" class="headerlink" title="个人心得"></a>个人心得</h1><p>上手Person Search相关的科研论文，这篇文章是必读的，是第一篇整合两个子任务到同一个框架的工作，源码由于是caffe的，对于新手不太友好，尤其是18年开始入坑CV的小白来说，一些更轻便的框架,把层，代价函数，模块封装好的，如tensorflow或是pytorch则更容易手上，建议可以从那个疑似复现代码的地址上手，可能复现不到这篇论文的结果，相差大约1~2%,但从熟悉整个Person Search的框架再好不过了，主要还是大部分的Person Search论文都没有开源，或者只是提供了test的部分代码和训练好的网络参数和结构，打包成了.pth等让读者在云盘上下载，有点小坑，很难复现到原始论文的结果。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文：Joint Detection and Identification Feature Learning for Person Search（整合行人检测和行人再识别的行人搜索新框架）&lt;br&gt;链接：&lt;a href=&quot;https://arxiv.org/abs/1604.01850&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1604.01850&lt;/a&gt;&lt;br&gt;代码地址： &lt;a href=&quot;https://github.com/ShuangLI59/person_search&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/ShuangLI59/person_search&lt;/a&gt; caffe版本&lt;br&gt;疑似复现地址：&lt;a href=&quot;https://github.com/liliangqi/person_search&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/liliangqi/person_search&lt;/a&gt; pytorch版本&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="论文笔记" scheme="http://gggcy.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="代码踩坑" scheme="http://gggcy.github.io/tags/%E4%BB%A3%E7%A0%81%E8%B8%A9%E5%9D%91/"/>
    
  </entry>
  
</feed>
